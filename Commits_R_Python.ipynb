{"cells":[{"metadata":{"_cell_guid":"a3c61050-dcd2-4cf7-a1b9-1f702b68e74e","_uuid":"74e0893f870ff79611b49162a438f9a62fe3e87d","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","plt.style.use('fivethirtyeight')\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","from subprocess import check_output\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4482aa89-2710-42b6-9b34-9c6a0230928a","_uuid":"201340a8e7efaff155f540f1b6bb729bfccc6e17"},"cell_type":"markdown","source":["### What is [BigQuery](https://cloud.google.com/bigquery/what-is-bigquery)??\n","\n","\n","Storing and querying massive datasets can be time consuming and expensive without the right hardware and infrastructure. Google BigQuery is an enterprise data warehouse that solves this problem by enabling super-fast SQL queries using the processing power of Google's infrastructure. Simply move your data into BigQuery and let us handle the hard work. You can control access to both the project and your data based on your business needs, such as giving others the ability to view or query your data.\n","\n","You can access BigQuery by using a [web UI ](https://bigquery.cloud.google.com/project/nice-particle-195309)or a [command-line tool](https://cloud.google.com/bigquery/bq-command-line-tool), or by making calls to the BigQuery REST API using a variety of client libraries such as Java, .NET, or Python. There are also a variety of third-party tools that you can use to interact with BigQuery, such as visualizing the data or loading the data.\n","Because the datasets on BigQuery can be very large, there are some restrictions on how much data you can access. \n","\n","*But You dont need to go to Google, Since Kaggle kernels allows you to access TeraBytes of data from Google cloud with all saftey measures like not letting your query go above memory limits and helper APIs. Thanks to [Sohier](https://www.kaggle.com/sohier)'s [BigQuery helper module](https://github.com/SohierDane/BigQuery_Helper/blob/master/bq_helper.py).\n","Each Kaggle user can scan 5TB every 30 days for free.*\n","\n","Let's first setup environment to run BigQueries in Kaggle Kernels.\n","\n","### Importing Kaggle's bq_helper package"]},{"metadata":{"_cell_guid":"a08640fc-a353-456b-b9a6-d8fdcc11e103","_uuid":"40a811126d0b55dee969c5340107592f8e4c8a34","trusted":true},"cell_type":"code","source":["import bq_helper "],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b6ad429-c606-4738-827c-2b08cc9ec70a","_uuid":"9b5f26f40dad9097966b335654b72c9d80ae3570"},"cell_type":"markdown","source":["### Creating a helper object for  bigquery dataset\n","\n","The addresses of BigQuery datasets look like this![](https://i.imgur.com/l11gdKx.png)\n","\n","for us dataset is **github_repos**\n","\n","[Rachael](https://www.kaggle.com/rtatman) from Kaggle has ran a 5 days BigQuery Introductory challenge called SQL Scavenger Hunt. We will go through day 1 to 5 using Github Repos Dataset.\n","\n","Image is taken from [SQL Scavenger Handbook](https://www.kaggle.com/rtatman/sql-scavenger-hunt-handbook)"]},{"metadata":{},"cell_type":"markdown","source":["In this case we are going to derive data from the monthy github archive datasets"]},{"metadata":{"_cell_guid":"48321d8d-51eb-416b-9f0c-034010ef0cd2","_uuid":"391415119f4c33697ff0ea72b6f74a7d8d97d79b","trusted":true},"cell_type":"code","source":["github_archive = bq_helper.BigQueryHelper(active_project= \"githubarchive\", \n","                                       dataset_name = \"month\")\n","\n","github_archive_day = bq_helper.BigQueryHelper(active_project= \"githubarchive\", \n","                                       dataset_name = \"day\")"],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26bf2aa8-3268-4b8f-b2f7-cfabbf82731b","_uuid":"f986ecd7fbe923e1ce95c0b475ded457032f60d7"},"cell_type":"markdown","source":["### Listing Tables"]},{"metadata":{"_cell_guid":"79b2227f-093e-4ff7-b288-e087e6ec3187","_uuid":"0de298b23d3282200e9ba1ffb20ddb93b6f5169a","trusted":true},"cell_type":"code","source":["# print a list of all the tables in the github archive dataset\n","github_archive.list_tables()"],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bad84731-af9f-4457-bead-75f7ea2ec940","_uuid":"f673434ae44c61970f02ac3de06c93bd7e3a242b"},"cell_type":"markdown","source":["###  Printing Table Schema\n"]},{"metadata":{"_cell_guid":"e9b9bbfd-af09-45e8-8d9a-c7b089752f0f","_uuid":"e3f038704629150caf10e59a0acf65c6b2a130b9","trusted":true},"cell_type":"code","source":["# print information on all the columns in the september 2015 dataset\n","# in the github_archive dataset\n","github_archive.table_schema(\"201509\")"],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9639c61-4f93-4ba1-ad9d-c12c7b173e61","_uuid":"576cc3603299b399800004fa71f06ba5ab9ab056","scrolled":true,"trusted":true},"cell_type":"code","source":["# preview the first couple lines of the table\n","github_archive.head(\"201509\")"],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9405a457-c3d3-491a-a808-f5fbec1e565a","_uuid":"0ed5b991e041fb8f795f1f18e6c9dc6346c85c7a"},"cell_type":"markdown","source":["### Checking the size of our query before we run it\n","\n","\n","\n","Our Dataset is quite large so we can easily cross tha monthly limit by running few queries. \n","\n","We should always estimate  how much data we need to scan for executing this query by **BigQueryHelper.estimate_query_size()** method.\n"]},{"metadata":{"_cell_guid":"2ad9394e-d741-44c2-9e23-c27486a291a6","_uuid":"5b22e98e2655624c4c4714e3e12d51e7a7df9ec9","scrolled":false,"trusted":true},"cell_type":"code","source":["query_pulls= \"\"\"SELECT\n","            repo,\n","            type,\n","            actor,\n","            payload\n","            FROM `githubarchive.month.201509`\n","            WHERE type = 'PullRequestEvent'\n","        \"\"\"\n","\n","query_pulls_user= \"\"\"SELECT\n","            repo.name as repo_name,\n","            JSON_EXTRACT(payload, '$.pull_request.user.login') as user,\n","            JSON_EXTRACT(payload, '$.pull_request.base.repo.language') as language\n","            FROM `githubarchive.day.20150901`\n","            WHERE type = 'PullRequestEvent'\n","            LIMIT 1000\n","        \"\"\"\n","\n","\n","query_commits_user= \"\"\"SELECT\n","            repo.name as repo_name,\n","            UNNEST(JSON_EXTRACT(payload, '$.commits')) as user,\n","            FROM `githubarchive.day.20150901`\n","            WHERE type = 'PushEvent'\n","            LIMIT 1000\n","        \"\"\"\n","\n","query_commits_user= \"\"\"SELECT\n","            repo.name as repo_name,\n","            JSON_EXTRACT(payload, '$.commits') as commits,\n","            actor.login as user_pull\n","            FROM `githubarchive.day.20150901`\n","            WHERE type = 'PushEvent'\n","            LIMIT 1000\n","        \"\"\"\n","\n","\n","github_archive.estimate_query_size(query_commits_user)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["github_repo_commits = github_archive.query_to_pandas_safe(query_commits_user, max_gb_scanned=2)\n","github_repo_commits.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["github_repo_commits.to_csv('github_repo_commits_20150901.csv',index=False)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["query1= \"\"\"SELECT\n","            repo,\n","            JSON_EXTRACT(payload, '$.pull_request.user.login') as puller,\n","            JSON_EXTRACT(payload, '$.pull_request.base.repo.language') as language\n","            FROM `githubarchive.month.201501`\n","            WHERE type = 'PullRequestEvent' AND JSON_EXTRACT(payload, '$.action.repo.language')\n","         \"\"\"\n","\n","query_pulls_user= \"\"\"SELECT\n","            repo.name as repo_name,\n","            JSON_EXTRACT(payload, '$.pull_request.user.login') as user,\n","            JSON_EXTRACT(payload, '$.pull_request.base.repo.language') as language\n","            FROM `githubarchive.month.201501`\n","            WHERE type = 'PullRequestEvent'\n","        \"\"\"\n","\n","query_pulls_user_grouped= \"\"\"SELECT\n","            JSON_EXTRACT(payload, '$.pull_request.user.login') AS user,\n","            repo.name AS repo_name,\n","            JSON_EXTRACT(payload, '$.pull_request.base.repo.language') AS language,\n","            COUNT(*) AS number_actions\n","            FROM `githubarchive.day.20150901`\n","            WHERE type = 'PullRequestEvent'\n","            GROUP BY repo_name, user, language\n","        \"\"\"\n","\n","# Where clause not working, dont know why\n","query_pulls_user_grouped_opened= \"\"\"SELECT\n","            JSON_EXTRACT(payload, '$.pull_request.user.login') AS user,\n","            repo.name AS repo_name,\n","            JSON_EXTRACT(payload, '$.pull_request.base.repo.language') AS language,\n","            COUNT(*) AS number_actions\n","            FROM `githubarchive.day.20150901`\n","            WHERE ((type = 'PullRequestEvent') AND (JSON_EXTRACT(payload, '$.action') = 'opened'))\n","            GROUP BY repo_name, user, language\n","        \"\"\"\n","\n","# This is the definitive query\n","query_pulls_user_grouped= \"\"\"SELECT\n","            JSON_EXTRACT(payload, '$.pull_request.user.login') AS user,\n","            repo.name AS repo_name,\n","            JSON_EXTRACT(payload, '$.pull_request.base.repo.language') AS language,\n","            JSON_EXTRACT(payload, '$.action') AS action,\n","            JSON_EXTRACT(payload, '$.pull_request.merged') AS merged,\n","            COUNT(*) AS number_actions\n","            FROM `githubarchive.day.20150901`\n","            WHERE type = 'PullRequestEvent'\n","            GROUP BY repo_name, user, language, action, merged\n","        \"\"\"\n","\n","\n","\n","github_archive_day.estimate_query_size(query_pulls_user_grouped)"],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7446e159-5be5-4f85-96eb-5ba975588cc8","_uuid":"11c94ec2dd2072a81fca0fd7a8ac518ff34956de"},"cell_type":"markdown","source":["Now I will use the query to pandas function to make the query of the dayly dataframe as a test"]},{"metadata":{"_cell_guid":"32b9aade-97a3-4a9a-abc5-1f904124dfab","_uuid":"560a098c6597a931b526a56240ae34592575dda7","trusted":true,"collapsed":true},"cell_type":"code","source":["github_user_pulls = github_archive_day.query_to_pandas_safe(query_pulls_user_grouped, max_gb_scanned=2)\n","github_user_pulls.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["github_user_pulls.to_csv('github_user_pull_20150901_actions.csv',index=False)"],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d85fea1-8a1a-4edb-918f-76a56beb701e","_uuid":"e74b08fa0b2ae15527144d65efd48ddd1d7c0f41"},"cell_type":"markdown","source":["Now I want to make a for loop that changes the query, extracting data from month to month during an specific year\n"]},{"metadata":{"_cell_guid":"260bebd2-d92f-4963-89b9-f8e70a9380f1","_uuid":"6a5e5e5894d073ba3d77db0f920b6a8c79c70430","trusted":true},"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","import bq_helper\n","\n","# Github archive dataset\n","github_archive = bq_helper.BigQueryHelper(active_project= \"githubarchive\", dataset_name = \"month\")\n","\n","#Arrays with the months and years under study\n","years_array = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"]\n","months_array = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\", \"11\",\"12\"]\n","\n","# Loop through the years and months\n","for query_year in years_array:           \n","    for query_month in months_array:\n","        \n","        # Print the year and month\n","        print(\"year = \" + query_year + \", month = \" + query_month)\n","        \n","        # Makes the query, it will change the database according to year and month\n","        query_pull_user= \"\"\"SELECT\n","                    actor.login as user,\n","                    repo.name AS repo_name,\n","                    COUNT(*) AS number_actions\n","                    FROM `githubarchive.month.{year}{month}`\n","                    WHERE type = 'PushEvent'\n","                    GROUP BY repo_name, user\n","                \"\"\".format(year= query_year,month=query_month)\n","\n","        \n","        # Executes the query and saves it to a mandas datafram, it will not work if more than 250 gb are scanned\n","        github_user_pulls = github_archive.query_to_pandas_safe(query_pull_user, max_gb_scanned=250)\n","        \n","        # Writes the csv\n","        github_user_pulls.to_csv(\"github_user_push_\" + query_year + query_month + \".csv\",index=False)\n","        \n","        print(\"Ready!\")\n","    "],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83d2c5cc-e162-444b-9418-1ac37235a214","_uuid":"031664196408266c7f862a0b2b8f472dbd9d1147","trusted":true,"collapsed":true},"cell_type":"code","source":["    # Query for the extraction of the number of pull actions in the dataframe\n","    query_push_user= \"\"\"SELECT\n","                actor.login as user\n","                repo.name AS repo_name,\n","                COUNT(*) AS number_actions\n","                FROM `githubarchive.month.{year}{month}`\n","                WHERE type = 'PushEvent'\n","                GROUP BY repo_name, user\n","            \"\"\".format(year= query_year,month=query_month)\n","    "],"execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60f53bfe-c8e4-4aca-8755-3b6b69433103","_uuid":"9c76de7d409bc99f1c89b277e2f7c8f39aca9aca"},"cell_type":"markdown","source":["### How many github repositories are in form of binary files?\n","A binary file is a file stored in binary format. A binary file is computer-readable but not human-readable. All executable programs are stored in binary files, as are most numeric data files."]},{"metadata":{"_cell_guid":"b785396e-8aa2-482f-ab69-6001a8522402","_uuid":"5e10911ad48c25cd8415c2855dec7dbc8cafd4db","trusted":true,"collapsed":true},"cell_type":"code","source":["#%%time\n","#query2= \"\"\"SELECT binary\n","#            FROM `bigquery-public-data.github_repos.contents`\n","#            LIMIT 50000\n","#        \"\"\"\n","#\n","#binary_files=github_repos.query_to_pandas_safe(query2)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}